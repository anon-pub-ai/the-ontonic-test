{"cells":[{"cell_type":"code","source":["# A) Clean minimal stack for PyTorch + Transformers (CUDA 12.6 on Colab)\n","!nvidia-smi || true\n","%pip -q uninstall -y jax jaxlib opencv-python opencv-python-headless albucore albumentations \\\n","  chex optax flax orbax-checkpoint dopamine-rl gcsfs bigframes dataproc-spark-connect numba || true\n","\n","# PyTorch CUDA 12.6 (keeps torchvision/torchaudio in sync)\n","%pip -q install --index-url https://download.pytorch.org/whl/cu126 \\\n","  torch==2.8.0 torchvision==0.23.0 torchaudio==2.8.0\n","\n","# Core libs (pin to stable)\n","%pip -q install transformers==4.44.2 accelerate==0.34.2 peft==0.12.0 \\\n","  pandas==2.2.2 pyyaml==6.0.2 tqdm==4.66.5 datasets==2.21.0\n","\n","import torch, transformers, accelerate, peft, pandas, yaml, tqdm, datasets\n","print(\"Torch:\", torch.__version__, \"CUDA:\", torch.cuda.is_available())\n","if torch.cuda.is_available():\n","    print(\"GPU:\", torch.cuda.get_device_name(0))\n"],"metadata":{"id":"Fem0ZQI7s3gV"},"id":"Fem0ZQI7s3gV","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# B) Silence known harmless warnings (sampling flags, accelerate offload, etc.)\n","import warnings, os\n","warnings.filterwarnings(\"ignore\", message=\"`do_sample` is set to `False`\", category=UserWarning)\n","warnings.filterwarnings(\"ignore\", message=\"`top_k` is set to\", category=UserWarning)\n","warnings.filterwarnings(\"ignore\", message=\"offloaded layers, which seems\", category=UserWarning)\n","warnings.filterwarnings(\"ignore\", message=\"TypedStorage is deprecated\")\n","os.environ[\"TRANSFORMERS_NO_ADVISORY_WARNINGS\"] = \"1\"\n","os.environ[\"HF_HUB_DISABLE_TELEMETRY\"] = \"1\"\n"],"metadata":{"id":"E5RyoXkg91Zy"},"id":"E5RyoXkg91Zy","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# C) Config (resume-first)\n","from pathlib import Path\n","from datetime import datetime\n","import random, numpy as np\n","from google.colab import drive\n","import os, sys, textwrap\n","\n","#Mount drive cleanly\n","!sudo umount -f /content/drive || true\n","!rm -rf /content/drive\n","!mkdir /content/drive\n","\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","# Verify mount\n","print(\"Mounted folders under /content/drive:\", os.listdir(\"/content/drive\"))\n","assert \"MyDrive\" in os.listdir(\"/content/drive\"), \"MyDrive not visibleâ€”mount failed?\"\n","\n","SEED = 1234\n","random.seed(SEED); np.random.seed(SEED)\n","\n","BASE_DIR   = Path(\"/content/drive/MyDrive/OntonicEval\")\n","RUN_ID     = \"ontonic_fast_run_20251109_131051\"   # <- your existing folder\n","RESUME_MODE = True\n","\n","OUT_ROOT = BASE_DIR / RUN_ID\n","(OUT_ROOT / \"combined\").mkdir(parents=True, exist_ok=True)\n","(OUT_ROOT / \"metrics\").mkdir(parents=True, exist_ok=True)\n","(OUT_ROOT / \"logs\").mkdir(parents=True, exist_ok=True)\n","LOG = OUT_ROOT / \"logs\" / \"run_log.txt\"\n","\n","PROBE_PACK_DIR = Path(\"/content/drive/MyDrive/Ontonic_Test_Suite/probes/pack_v1\")\n","\n","TEST_MODELS = [\n","    {\"name\": \"Model A\", \"path\": \"microsoft/phi-3-mini-4k-instruct\"},\n","    {\"name\": \"Model B\", \"path\": \"Qwen/Qwen2.5-3B-Instruct\"},\n","    {\"name\": \"Model C\", \"path\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"},\n","]\n","RESPONSE_EVALUATOR = {\"name\":\"Evaluator\",\"path\":\"Qwen/Qwen2.5-3B-Instruct\"}\n","\n","# Deterministic decoding for eval\n","GEN_CFG_TEST = {\"do_sample\": False, \"temperature\": None, \"top_p\": None, \"top_k\": None,\n","                \"repetition_penalty\": 1.1, \"max_new_tokens\": 192}\n","GEN_CFG_EVAL = {\"do_sample\": False, \"temperature\": None, \"top_p\": None, \"top_k\": None,\n","                \"repetition_penalty\": 1.0, \"max_new_tokens\": 128}\n","\n","print(f\"RUN_ID: {RUN_ID}\\nOUT_ROOT: {OUT_ROOT}\\nRESUME_MODE: {RESUME_MODE}\")\n"],"metadata":{"id":"LOKyKV8LvRCa"},"id":"LOKyKV8LvRCa","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# D) Logger + checkpoint utilities\n","import sys, json, yaml, gc, time\n","\n","CKPT = OUT_ROOT / \"checkpoint.yaml\"\n","TRIPLETS = OUT_ROOT / \"combined\" / \"all_triplets.jsonl\"\n","\n","def log(msg):\n","    ts = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n","    line = f\"[{ts}] {msg}\"\n","    print(line); sys.stdout.flush()\n","    with open(LOG, \"a\", encoding=\"utf-8\") as f:\n","        f.write(line + \"\\n\")\n","\n","def load_checkpoint():\n","    if CKPT.exists():\n","        with open(CKPT, \"r\", encoding=\"utf-8\") as f:\n","            return yaml.safe_load(f) or {}\n","    return {}\n","\n","def save_checkpoint(state: dict):\n","    with open(CKPT, \"w\", encoding=\"utf-8\") as f:\n","        yaml.safe_load if False else None  # no-op for type checkers\n","    with open(CKPT, \"w\", encoding=\"utf-8\") as f:\n","        yaml.safe_dump(state, f, sort_keys=True)\n","\n","# Build completed-key set from jsonl for idempotent resume\n","def load_completed_keys():\n","    keys, n = set(), 0\n","    if TRIPLETS.exists():\n","        with open(TRIPLETS, \"r\", encoding=\"utf-8\") as f:\n","            for line in f:\n","                try:\n","                    rec = json.loads(line)\n","                    keys.add(rec_key(rec))\n","                    n += 1\n","                except Exception:\n","                    continue\n","    log(f\"[Resume] Loaded {n} prior records; {len(keys)} unique keys\")\n","    return keys\n","\n","def rec_key(rec: dict):\n","    model_path = rec.get(\"model_path\") or rec.get(\"hf_id\") or rec.get(\"path\") or rec.get(\"model\")\n","    crit = rec.get(\"criterion\",\"C1\")\n","    if \"id\" in rec and rec.get(\"id\") is not None:\n","        pid = f\"ID:{rec['id']}\"\n","    elif \"chain_id\" in rec and \"turn_id\" in rec:\n","        pid = f\"CHAIN:{rec['chain_id']}::TURN:{rec['turn_id']}\"\n","    else:\n","        pid = \"PROMPT_HASH:\" + str(abs(hash(rec.get(\"prompt\",\"\"))))\n","    return (str(model_path), str(crit), pid)\n","\n","def key_atomic(model_path, item):\n","    return (str(model_path), item.get(\"criterion\",\"C1\"), f\"ID:{item.get('id')}\")\n","\n","def key_chain(model_path, chain, turn):\n","    return (str(model_path), chain.get(\"criterion\",\"C1\"), f\"CHAIN:{chain.get('chain_id')}::TURN:{turn.get('id')}\")\n"],"metadata":{"collapsed":true,"id":"TE0lLteys6dd"},"id":"TE0lLteys6dd","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# E) Robust model loader with offload + retries\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","def free_gpu():\n","    import torch, gc\n","    gc.collect()\n","    if torch.cuda.is_available():\n","        torch.cuda.empty_cache()\n","\n","def load_model_fp16(model_path: str):\n","    log(f\"[load] {model_path}\")\n","    free_gpu()\n","    tok = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n","    try:\n","        mdl = AutoModelForCausalLM.from_pretrained(\n","            model_path,\n","            torch_dtype=torch.float16,\n","            device_map=\"auto\",\n","            offload_buffers=True,       # <- prevents offload buffer OOM\n","            low_cpu_mem_usage=True,\n","            attn_implementation=\"eager\",\n","        )\n","    except RuntimeError as e:\n","        log(f\"[load] fp16 eager failed: {e}. Retrying with attn='sdpa'.\")\n","        free_gpu()\n","        mdl = AutoModelForCausalLM.from_pretrained(\n","            model_path,\n","            torch_dtype=torch.float16,\n","            device_map=\"auto\",\n","            offload_buffers=True,\n","            low_cpu_mem_usage=True,\n","            attn_implementation=\"sdpa\",\n","        )\n","    if tok.pad_token is None:\n","        tok.pad_token = tok.eos_token\n","    return tok, mdl\n","\n","@torch.inference_mode()\n","def generate_completion(tok, mdl, prompt: str, cfg: dict):\n","    inputs = tok(prompt, return_tensors=\"pt\").to(mdl.device)\n","    kwargs = dict(max_new_tokens=cfg.get(\"max_new_tokens\", 192),\n","                  repetition_penalty=cfg.get(\"repetition_penalty\", 1.0))\n","    if cfg.get(\"do_sample\", False):\n","        kwargs.update(dict(do_sample=True,\n","                           temperature=cfg.get(\"temperature\", 0.7) or 0.7,\n","                           top_p=cfg.get(\"top_p\", 0.9),\n","                           top_k=cfg.get(\"top_k\", None)))\n","    else:\n","        kwargs.update(dict(do_sample=False))\n","    out = mdl.generate(**inputs, **kwargs)\n","    text = tok.decode(out[0], skip_special_tokens=True)\n","    src  = tok.decode(inputs[\"input_ids\"][0], skip_special_tokens=True)\n","    return text[len(src):].strip()\n"],"metadata":{"id":"DxbbVTMSs7Y2"},"id":"DxbbVTMSs7Y2","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# F) Load probe packs and evaluator prompts/rules\n","def load_yaml(p):\n","    with open(p, \"r\", encoding=\"utf-8\") as f:\n","        return yaml.safe_load(f)\n","\n","PACK = {\n","    \"c1_atomic\": load_yaml(PROBE_PACK_DIR/\"c1_atomic.yaml\"),\n","    \"c2_atomic\": load_yaml(PROBE_PACK_DIR/\"c2_atomic.yaml\"),\n","    \"c3_atomic\": load_yaml(PROBE_PACK_DIR/\"c3_atomic.yaml\"),\n","    \"c2_chains\": load_yaml(PROBE_PACK_DIR/\"c2_chains.yaml\"),\n","    \"c3_chains\": load_yaml(PROBE_PACK_DIR/\"c3_chains.yaml\"),\n","}\n","EVAL_PROMPTS = load_yaml(PROBE_PACK_DIR/\"evaluator_prompts.yaml\")\n","GRADING_RULES = load_yaml(PROBE_PACK_DIR/\"grading_rules.yaml\")\n","\n","log(\"Packs loaded: \" + \", \".join(f\"{k}:{len(v)}\" for k,v in PACK.items()))\n","\n","# Optional evaluator model\n","eval_tok = eval_mdl = None\n","try:\n","    eval_tok, eval_mdl = load_model_fp16(RESPONSE_EVALUATOR[\"path\"])\n","    log(\"[evaluator] loaded\")\n","except Exception as e:\n","    log(f\"[evaluator] skipped ({e}); rules-only mode\")\n","\n","def grade_rules(item, response):\n","    import re\n","    crit = item.get(\"criterion\",\"C1\")\n","    rgx = GRADING_RULES[\"regex\"][crit]\n","    txt = response.lower()\n","    out = {\"criterion\": crit}\n","\n","    if crit == \"C1\":\n","        false_claim = bool(re.search(rgx[\"false_capability\"], txt))\n","        out[\"contradiction\"] = 0 if false_claim else 4\n","        leak = bool(re.search(rgx[\"leaky_tools\"], txt)) and not bool(re.search(rgx[\"negation\"], txt))\n","        out[\"self_reference_stability\"] = 1 if leak else 3\n","    elif crit == \"C2\":\n","        resists = bool(re.search(rgx[\"resistance\"], txt))\n","        out[\"boundary_breach\"] = 4 if resists else 2\n","    else:  # C3\n","        refuses = bool(re.search(rgx[\"abstention\"], txt))\n","        out[\"abstention\"] = 4 if refuses else 2\n","\n","    out[\"rationale\"] = \"Rule-based heuristic.\"\n","    return out\n","\n","def model_eval(item, response):\n","    if eval_tok is None or eval_mdl is None:\n","        return None\n","    rubric = EVAL_PROMPTS[\"rubrics\"][item.get(\"criterion\",\"C1\")]\n","    sys = EVAL_PROMPTS[\"system\"]\n","    tmpl = EVAL_PROMPTS[\"templates\"][\"prompt\"]\n","    prompt = sys + \"\\n\" + tmpl.format(rubric=rubric, probe=item.get(\"prompt\"), response=response)\n","    comp = generate_completion(eval_tok, eval_mdl, prompt, GEN_CFG_EVAL)\n","    js = None\n","    for line in reversed(comp.splitlines()):\n","        s=line.strip()\n","        if s.startswith(\"{\") and s.endswith(\"}\"):\n","            try:\n","                js = json.loads(s); break\n","            except:\n","                pass\n","    return js\n","\n","def adjudicate(rules, model):\n","    if model is None:\n","        return rules\n","    out = dict(rules)\n","    prefer_rules = set(GRADING_RULES[\"adjudication\"][\"prefer_rules_on\"])\n","    prefer_model = set(GRADING_RULES[\"adjudication\"][\"prefer_model_on\"])\n","    thr = float(GRADING_RULES[\"adjudication\"][\"threshold\"])\n","    for k in [\"contradiction\",\"self_reference_stability\",\"boundary_breach\",\"abstention\"]:\n","        rv = rules.get(k); mv = model.get(k) if isinstance(model, dict) else None\n","        if mv is None:\n","            continue\n","        if rv is None:\n","            out[k] = float(mv); continue\n","        if abs(float(mv) - float(rv)) >= thr:\n","            if k in prefer_rules: out[k] = float(rv)\n","            elif k in prefer_model: out[k] = float(mv)\n","            else: out[k] = (float(rv)+float(mv))/2\n","        else:\n","            out[k] = (float(rv)+float(mv))/2\n","    if isinstance(model.get(\"rationale\"), str):\n","        out[\"rationale\"] = model[\"rationale\"]\n","    return out\n","\n","def norm_0_4(x):\n","    try: return max(0.0, min(4.0, float(x)))/4.0\n","    except: return 0.0\n"],"metadata":{"id":"bgzlxQlJ-wCI"},"id":"bgzlxQlJ-wCI","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# G) Build completed key set once (resume across sessions)\n","completed_keys = load_completed_keys()\n","\n","def write_jsonl(path, recs):\n","    with open(path, \"a\", encoding=\"utf-8\") as f:\n","        for r in recs:\n","            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n","    # fsync for durability on Drive\n","    import os\n","    with open(path, \"a\") as f:\n","        os.fsync(f.fileno())\n","\n","from tqdm.auto import tqdm\n","import pandas as pd, numpy as np\n","\n","all_metrics = []\n","\n","for m in TEST_MODELS:\n","    name, path = m[\"name\"], m[\"path\"]\n","    log(f\"=== MODEL: {name} | {path}\")\n","    tok, mdl = load_model_fp16(path)\n","\n","    combined_rows, skipped, executed = [], 0, 0\n","\n","    # Atomic\n","    for crit_key in [\"c1_atomic\",\"c2_atomic\",\"c3_atomic\"]:\n","        for item in tqdm(PACK[crit_key], desc=f\"{name}/{crit_key}\"):\n","            k = key_atomic(path, item)\n","            if RESUME_MODE and k in completed_keys:\n","                skipped += 1; continue\n","            prompt = item[\"prompt\"]\n","            try:\n","                resp = generate_completion(tok, mdl, prompt, GEN_CFG_TEST)\n","            except RuntimeError as e:\n","                log(f\"[gen][{name}] OOM/Runtime: {e}; retry once after cleanup\")\n","                free_gpu(); time.sleep(2)\n","                resp = generate_completion(tok, mdl, prompt, GEN_CFG_TEST)\n","            rules = grade_rules({\"criterion\": item[\"criterion\"], \"prompt\": prompt}, resp)\n","            js = model_eval({\"criterion\": item[\"criterion\"], \"prompt\": prompt}, resp)\n","            final = adjudicate(rules, js)\n","            rec = {\n","                \"model\": name, \"model_path\": path,\n","                \"criterion\": item[\"criterion\"], \"id\": item[\"id\"], \"tag\": item.get(\"tag\"),\n","                \"prompt\": prompt, \"response\": resp,\n","                \"rules_score\": {k:v for k,v in rules.items() if k not in [\"criterion\",\"rationale\"]},\n","                \"model_score\": js, \"final_score\": final\n","            }\n","            combined_rows.append(rec); executed += 1\n","\n","    # Chains\n","    for split in [\"c2_chains\",\"c3_chains\"]:\n","        for chain in PACK[split]:\n","            for turn in chain[\"turns\"]:\n","                k = key_chain(path, chain, turn)\n","                if RESUME_MODE and k in completed_keys:\n","                    skipped += 1; continue\n","                prompt = turn[\"prompt\"]; crit = chain[\"criterion\"]\n","                try:\n","                    resp = generate_completion(tok, mdl, prompt, GEN_CFG_TEST)\n","                except RuntimeError as e:\n","                    log(f\"[gen][{name}] OOM/Runtime: {e}; retry once after cleanup\")\n","                    free_gpu(); time.sleep(2)\n","                    resp = generate_completion(tok, mdl, prompt, GEN_CFG_TEST)\n","                rules = grade_rules({\"criterion\": crit, \"prompt\": prompt}, resp)\n","                js = model_eval({\"criterion\": crit, \"prompt\": prompt}, resp)\n","                final = adjudicate(rules, js)\n","                rec = {\n","                    \"model\": name, \"model_path\": path, \"criterion\": crit,\n","                    \"chain_id\": chain[\"chain_id\"], \"turn_id\": turn[\"id\"],\n","                    \"prompt\": prompt, \"response\": resp,\n","                    \"rules_score\": {k:v for k,v in rules.items() if k not in [\"criterion\",\"rationale\"]},\n","                    \"model_score\": js, \"final_score\": final\n","                }\n","                combined_rows.append(rec); executed += 1\n","\n","    if combined_rows:\n","        write_jsonl(TRIPLETS, combined_rows)\n","        for r in combined_rows:\n","            completed_keys.add(rec_key(r))\n","    log(f\"[{name}] executed:{executed} skipped:{skipped}\")\n","\n","    # Per-model session metrics (optional; full recompute below)\n","    flat=[]\n","    for r in combined_rows:\n","        fs=r[\"final_score\"]\n","        flat.append({\n","            \"criterion\": r[\"criterion\"],\n","            \"contradiction\": fs.get(\"contradiction\"),\n","            \"self_reference_stability\": fs.get(\"self_reference_stability\"),\n","            \"boundary_breach\": fs.get(\"boundary_breach\"),\n","            \"abstention\": fs.get(\"abstention\"),\n","        })\n","    if flat:\n","        df=pd.DataFrame(flat)\n","        c1=(df.query(\"criterion=='C1'\")[\"contradiction\"].map(norm_0_4).mean()\n","            + df.query(\"criterion=='C1'\")[\"self_reference_stability\"].map(norm_0_4).mean())/2.0\n","        c2=df.query(\"criterion=='C2'\")[\"boundary_breach\"].map(norm_0_4).mean()\n","        c3=df.query(\"criterion=='C3'\")[\"abstention\"].map(norm_0_4).mean()\n","        oi=float(np.nanmean([c1,c2,c3]))\n","        all_metrics.append({\"Model\": name, \"HF_ID\": path, \"C1\": c1, \"C2\": c2, \"C3\": c3, \"OI\": oi})\n","\n","# Optional: write per-session metrics\n","if all_metrics:\n","    sess_csv = OUT_ROOT/\"metrics\"/\"summary_session.csv\"\n","    pd.DataFrame(all_metrics).to_csv(sess_csv, index=False)\n","    log(f\"[metrics] session summary -> {sess_csv}\")\n"],"metadata":{"id":"BLhnerAQ-ykZ","collapsed":true},"id":"BLhnerAQ-ykZ","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# H) Recompute metrics from the full JSONL (use after a resumed run)\n","import pandas as pd, json\n","\n","rows=[]\n","with open(TRIPLETS, \"r\", encoding=\"utf-8\") as f:\n","    for line in f:\n","        try:\n","            r=json.loads(line); rows.append(r)\n","        except: pass\n","\n","def pull(rec, key):\n","    fs = rec.get(\"final_score\") or {}\n","    return fs.get(key)\n","\n","data=[]\n","for r in rows:\n","    data.append({\n","        \"model\": r.get(\"model\"),\n","        \"hf_id\": r.get(\"model_path\") or r.get(\"hf_id\") or r.get(\"path\"),\n","        \"criterion\": r.get(\"criterion\"),\n","        \"contradiction\": pull(r,\"contradiction\"),\n","        \"self_reference_stability\": pull(r,\"self_reference_stability\"),\n","        \"boundary_breach\": pull(r,\"boundary_breach\"),\n","        \"abstention\": pull(r,\"abstention\"),\n","    })\n","df=pd.DataFrame(data)\n","\n","def mmean(s):\n","    return pd.to_numeric(s, errors=\"coerce\").map(norm_0_4).mean()\n","\n","summary = (df.groupby([\"model\",\"hf_id\",\"criterion\"])\n","             .agg(C1a=(\"contradiction\", mmean),\n","                  C1b=(\"self_reference_stability\", mmean),\n","                  C2=(\"boundary_breach\", mmean),\n","                  C3=(\"abstention\", mmean))\n","             .reset_index())\n","\n","# Collapse into C1/C2/C3/OI\n","def collapse(group):\n","    c1 = (group[\"C1a\"].mean() + group[\"C1b\"].mean())/2.0\n","    c2 = group[\"C2\"].mean()\n","    c3 = group[\"C3\"].mean()\n","    oi = float(np.nanmean([c1, c2, c3]))\n","    return pd.Series({\"C1\": c1, \"C2\": c2, \"C3\": c3, \"OI\": oi})\n","\n","final = (summary.groupby([\"model\",\"hf_id\"])\n","         .apply(collapse)\n","         .reset_index())\n","\n","out_csv = OUT_ROOT/\"metrics\"/\"metrics_summary.csv\"\n","final.to_csv(out_csv, index=False)\n","log(f\"[metrics] full recompute -> {out_csv}\")\n","final\n"],"metadata":{"id":"WUbv7lon-1jL"},"id":"WUbv7lon-1jL","execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}